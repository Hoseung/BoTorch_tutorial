{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPyTorch Regression Tutorial\n",
    "\n",
    "Large part of BoTorch's tutorial utilize GPyTorch, as Gaussian Process is often the model of choice in Bayesian Optimization tasks. I am not sure I absolutely need this, but familiarizing myself with it won't hurt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a RBF kernel Gaussian process on a simple function\n",
    "\n",
    "$$ y = sin(2πx)+ϵ \\\\ \n",
    "error \\quad ϵ∼N(0,0.04)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.linspace(0, 1, 100)\n",
    "# output with some error\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * math.sqrt(0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "GPyTorch provides not a list of pre-defined GP models, but the necessary tools to build all kind of GP models for maximum flexibility.\n",
    "\n",
    "A complete model should include\n",
    "1. A GP Model  \n",
    "    *`gpytorch.models.ExactGP`, for example\n",
    "2. A Likelihood   \n",
    "    * `gpytorch.likelihoods.GaussianLikelihood`, for example\n",
    "3. A Mean  \n",
    "    * `gpytorch.means.ConstantMean`, for example)\n",
    "4. A Kernel: defines the prior covariance of the GP  \n",
    "    * `gpytorch.kernels.Scalekernel(gpytorch.kernels.RBFKernel())`, for example\n",
    "5. A MultivariateNormal \n",
    "    * `gpytorch.distributions.MultivariateNormal` for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel # Radial Basis Function\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.priors import GammaPrior\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        # Does some set up:\n",
    "        # self.model_inputs = train_x \n",
    "        # self.model_targets = train_y\n",
    "        # self.likelihood = likelihood\n",
    "        \n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel()) \n",
    "        # Remember the form of covariance determines the characteristics of\n",
    "        # fitted function. from chaotic spiky to smooth / straight.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "likelihood = GaussianLikelihood() # Assumes homoskedastic noise model\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "homoscedasticity == \"same variance\"\n",
    "\n",
    "Like most PyTorch module, ExactGP has .train() mode for training and .eval() mode for computing predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Like in PyTorch, the training loop has to be written by the user.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.926   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/50 - Loss: 0.894   lengthscale: 0.644   noise: 0.644\n",
      "Iter 3/50 - Loss: 0.860   lengthscale: 0.598   noise: 0.598\n",
      "Iter 4/50 - Loss: 0.822   lengthscale: 0.555   noise: 0.554\n",
      "Iter 5/50 - Loss: 0.781   lengthscale: 0.514   noise: 0.513\n",
      "Iter 6/50 - Loss: 0.735   lengthscale: 0.475   noise: 0.474\n",
      "Iter 7/50 - Loss: 0.686   lengthscale: 0.438   noise: 0.437\n",
      "Iter 8/50 - Loss: 0.637   lengthscale: 0.404   noise: 0.402\n",
      "Iter 9/50 - Loss: 0.590   lengthscale: 0.371   noise: 0.370\n",
      "Iter 10/50 - Loss: 0.546   lengthscale: 0.341   noise: 0.339\n",
      "Iter 11/50 - Loss: 0.507   lengthscale: 0.315   noise: 0.311\n",
      "Iter 12/50 - Loss: 0.470   lengthscale: 0.292   noise: 0.284\n",
      "Iter 13/50 - Loss: 0.435   lengthscale: 0.272   noise: 0.260\n",
      "Iter 14/50 - Loss: 0.401   lengthscale: 0.256   noise: 0.237\n",
      "Iter 15/50 - Loss: 0.368   lengthscale: 0.244   noise: 0.216\n",
      "Iter 16/50 - Loss: 0.335   lengthscale: 0.234   noise: 0.197\n",
      "Iter 17/50 - Loss: 0.303   lengthscale: 0.227   noise: 0.179\n",
      "Iter 18/50 - Loss: 0.271   lengthscale: 0.223   noise: 0.163\n",
      "Iter 19/50 - Loss: 0.239   lengthscale: 0.220   noise: 0.148\n",
      "Iter 20/50 - Loss: 0.207   lengthscale: 0.219   noise: 0.135\n",
      "Iter 21/50 - Loss: 0.177   lengthscale: 0.220   noise: 0.123\n",
      "Iter 22/50 - Loss: 0.147   lengthscale: 0.223   noise: 0.112\n",
      "Iter 23/50 - Loss: 0.119   lengthscale: 0.227   noise: 0.101\n",
      "Iter 24/50 - Loss: 0.092   lengthscale: 0.232   noise: 0.092\n",
      "Iter 25/50 - Loss: 0.067   lengthscale: 0.239   noise: 0.084\n",
      "Iter 26/50 - Loss: 0.045   lengthscale: 0.246   noise: 0.077\n",
      "Iter 27/50 - Loss: 0.025   lengthscale: 0.254   noise: 0.070\n",
      "Iter 28/50 - Loss: 0.008   lengthscale: 0.263   noise: 0.064\n",
      "Iter 29/50 - Loss: -0.006   lengthscale: 0.272   noise: 0.059\n",
      "Iter 30/50 - Loss: -0.016   lengthscale: 0.281   noise: 0.055\n",
      "Iter 31/50 - Loss: -0.024   lengthscale: 0.290   noise: 0.050\n",
      "Iter 32/50 - Loss: -0.028   lengthscale: 0.298   noise: 0.047\n",
      "Iter 33/50 - Loss: -0.029   lengthscale: 0.304   noise: 0.044\n",
      "Iter 34/50 - Loss: -0.028   lengthscale: 0.308   noise: 0.041\n",
      "Iter 35/50 - Loss: -0.026   lengthscale: 0.310   noise: 0.039\n",
      "Iter 36/50 - Loss: -0.023   lengthscale: 0.310   noise: 0.037\n",
      "Iter 37/50 - Loss: -0.019   lengthscale: 0.307   noise: 0.035\n",
      "Iter 38/50 - Loss: -0.017   lengthscale: 0.302   noise: 0.034\n",
      "Iter 39/50 - Loss: -0.014   lengthscale: 0.297   noise: 0.033\n",
      "Iter 40/50 - Loss: -0.012   lengthscale: 0.290   noise: 0.032\n",
      "Iter 41/50 - Loss: -0.010   lengthscale: 0.284   noise: 0.032\n",
      "Iter 42/50 - Loss: -0.010   lengthscale: 0.277   noise: 0.032\n",
      "Iter 43/50 - Loss: -0.010   lengthscale: 0.271   noise: 0.032\n",
      "Iter 44/50 - Loss: -0.011   lengthscale: 0.266   noise: 0.032\n",
      "Iter 45/50 - Loss: -0.012   lengthscale: 0.262   noise: 0.032\n",
      "Iter 46/50 - Loss: -0.014   lengthscale: 0.258   noise: 0.033\n",
      "Iter 47/50 - Loss: -0.017   lengthscale: 0.256   noise: 0.034\n",
      "Iter 48/50 - Loss: -0.019   lengthscale: 0.255   noise: 0.034\n",
      "Iter 49/50 - Loss: -0.022   lengthscale: 0.254   noise: 0.035\n",
      "Iter 50/50 - Loss: -0.024   lengthscale: 0.255   noise: 0.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoseung/anaconda3/envs/tm38/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    }
   ],
   "source": [
    "# Set the model and likelihood to training mode.\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Just like usual PyTorch network model.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# \"loss\" for GP\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(train_x)\n",
    "    \n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
